{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qCH99T-XWg0",
        "outputId": "5739f60d-a536-4073-8506-300363d33ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MASTER_PATH = \"/content/drive/MyDrive/BOUN COURSES/SWE 522/02-PROJECTS/Project-2/FR-Refactor-IMPLEMENTED\""
      ],
      "metadata": {
        "id": "_F42UhMzXMz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE DATASET\n",
        "\n"
      ],
      "metadata": {
        "id": "CGRDhcCaHa7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    f = open(MASTER_PATH + '/src/dataset/final_dataset.pickle', 'rb')\n",
        "    dataset = pickle.load(f)\n",
        "    f.close()\n",
        "    print(\"dataset laoded.....\")\n",
        "    print('==========>')\n",
        "\n",
        "\n",
        "    binary_class_dataset = []\n",
        "    for index in range(len(dataset)):\n",
        "        task = []\n",
        "        task.append(dataset[index][0])\n",
        "        if isinstance(dataset[index][1], int):\n",
        "          ref = dataset[index][1]\n",
        "        else:\n",
        "          if str(dataset[index][1][0]).strip() == 'none':\n",
        "            ref = 0\n",
        "          else:\n",
        "            ref = dataset[index][1]\n",
        "        task.append(ref)\n",
        "\n",
        "        binary_class_dataset.append(task)\n",
        "\n",
        "    f = open(MASTER_PATH + '/src/dataset/final_dataset.pickle', 'wb')\n",
        "    pickle.dump(binary_class_dataset,f)\n",
        "    f.close()\n",
        "    print(\"dataset saved.....\")\n",
        "    print('==========>')\n",
        "\n",
        "    print(binary_class_dataset[0])\n"
      ],
      "metadata": {
        "id": "tcnmQVr9Heha",
        "outputId": "0e64a60b-bfde-4a08-ee37-321aa777c8d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset laoded.....\n",
            "==========>\n",
            "dataset saved.....\n",
            "==========>\n",
            "[' anoth blueprint sampl contribut day test blueprint mbean gener complex blueprint sampl sampl user learn defin nest compon blueprint xml run sampl besid osgi framework bundl requir coreopt mavenbundl groupid org apach felix artifactid org apach felix configadmin versionasinproject coreopt mavenbundl groupid org apach felix artifactid org apach felix eventadmin versionasinproject coreopt mavenbundl groupid org ops4j pax log artifactid pax log api versionasinproject coreopt mavenbundl groupid org ops4j pax log artifactid pax log servic versionasinproject coreopt mavenbundl groupid org apach ari blueprint artifactid org apach ari blueprint versionasinproject coreopt mavenbundl groupid org apach ari artifactid org apach ari util coreopt mavenbundl groupid org apach ari jmx artifactid ari jmx blueprint versionasinproject', 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "feature_words.py\n"
      ],
      "metadata": {
        "id": "PHJ8lYBLHvGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYIZQERQBeYe",
        "outputId": "41d43230-e563-4da6-baf9-e6da1992a7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sne3Kg-ftx1z",
        "outputId": "ba2cf7bd-c44f-479c-b3b6-4f705d3d1d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2022-12-20 13:27:30.838744: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "Ikxa00Nht2xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import re\n",
        "from collections import Counter\n",
        "import string\n",
        "import numpy as np\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    return nlp.make_doc(text).text\n",
        "\n",
        "def tokenization(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "\n",
        "def pos_tagging(text):\n",
        "    doc = nlp(text)\n",
        "    return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "\n",
        "def lemmatization(pos_tags):\n",
        "    lemmatized_text = []\n",
        "    for word in pos_tags:\n",
        "        lemma = nlp(word[0])[0].lemma_\n",
        "        lemmatized_text.append(lemma)\n",
        "    return lemmatized_text\n",
        "\n",
        "\n",
        "# implementation\n",
        "def stemming(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_text = [stemmer.stem(word) for word in text]\n",
        "    return stemmed_text\n",
        "\n",
        "def stop_word_removal(pos_tags_lem, lem_text):\n",
        "    doc = nlp(lem_text)\n",
        "    processed_text = [token.text for token in doc if not token.is_stop]\n",
        "    stopwords = set(token.text for token in doc if token.is_stop)\n",
        "    return processed_text, stopwords\n",
        "\n",
        "if __name__ == '__main__' :\n",
        "\n",
        "    load = open(MASTER_PATH + \"/src/dataset/binary_class_dataset.pickle\", \"rb\")\n",
        "    tasks = pickle.load(load)\n",
        "    load.close()\n",
        "    print(tasks[0])\n",
        "    print(\"tasks loaded.....\")\n",
        "\n",
        "    preprocessed_tasks = []\n",
        "\n",
        "    all_words = []\n",
        "    freq_words_without_freq = []\n",
        "\n",
        "    for index in range(len(tasks)):\n",
        "\n",
        "        # without preprocessing\n",
        "        old_task = tasks[index]\n",
        "        text = tasks[index][0].lower()\n",
        "        cleaned_text = clean_text(text)\n",
        "        #!!!IMPLEMENTED!!!\n",
        "        tokens = tokenization(cleaned_text)\n",
        "        stemmed_tokens = stemming(tokens)\n",
        "        stemmed_text = \" \".join(stemmed_tokens)\n",
        "        pos_tags = pos_tagging(stemmed_text)\n",
        "        lemmatized_text = lemmatization(pos_tags)\n",
        "        #words = re.findall(r'\\w+', cleaned_text)\n",
        "        for item in lemmatized_text:\n",
        "          all_words.append(item)\n",
        "\n",
        "        # without preprocessing\n",
        "        #for item in words:\n",
        "\n",
        "            #all_words.append(item)\n",
        "\n",
        "    counts = Counter(all_words).most_common(5000)\n",
        "\n",
        "    for index in range(len(counts)):\n",
        "        freq_words_without_freq.append(counts[index][0])\n",
        "\n",
        "\n",
        "    file = open(MASTER_PATH + \"/src/dataset/freq_words_without_preprocessing.pickle\", \"wb\")\n",
        "    pickle.dump(freq_words_without_freq, file)\n",
        "    file.close()\n",
        "    print(\"all_phrases_without_freq saved......\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcil4FjLuJOO",
        "outputId": "c7b635cc-8840-4209-8e5b-6fca853ae449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' anoth blueprint sampl contribut day test blueprint mbean gener complex blueprint sampl sampl user learn defin nest compon blueprint xml run sampl besid osgi framework bundl requir coreopt mavenbundl groupid org apach felix artifactid org apach felix configadmin versionasinproject coreopt mavenbundl groupid org apach felix artifactid org apach felix eventadmin versionasinproject coreopt mavenbundl groupid org ops4j pax log artifactid pax log api versionasinproject coreopt mavenbundl groupid org ops4j pax log artifactid pax log servic versionasinproject coreopt mavenbundl groupid org apach ari blueprint artifactid org apach ari blueprint versionasinproject coreopt mavenbundl groupid org apach ari artifactid org apach ari util coreopt mavenbundl groupid org apach ari jmx artifactid ari jmx blueprint versionasinproject', 0]\n",
            "tasks loaded.....\n",
            "all_phrases_without_freq saved......\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. featuresets_generation.py"
      ],
      "metadata": {
        "id": "bQ9iSOvrJgv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import random\n",
        "\n",
        "def load_dataset():\n",
        "    file = open(MASTER_PATH + \"/src/dataset/shuffle_dataset.pickle\", \"rb\")\n",
        "    dataset = pickle.load(file)\n",
        "    file.close()\n",
        "    print(\"dataset of length \" , len(dataset) , \" is loaded.....\")\n",
        "    return dataset\n",
        "\n",
        "def load_features():\n",
        "    # file = open(\"./dataset/features.pickle\", \"rb\")\n",
        "    file = open(MASTER_PATH + \"/src/dataset/features_without_pp.pickle\", \"rb\")\n",
        "    features = pickle.load(file)\n",
        "    file.close()\n",
        "    print(\"features of length \" , len(features) , \" is loaded.....\")\n",
        "    # print(features[:100])\n",
        "    return features\n",
        "\n",
        "def find_features(w_features, t_words):\n",
        "    words = t_words\n",
        "    features = {}\n",
        "    for f in w_features:\n",
        "        features[f] = 0\n",
        "\n",
        "    for f in features:\n",
        "        if f in words:\n",
        "            features[f] += 1\n",
        "    return features\n",
        "\n",
        "\n",
        "def make_featuresets(word_features):\n",
        "\n",
        "    documents_f = open(MASTER_PATH + \"/src/T&T/testing.pickle\", \"rb\")\n",
        "    testing_files = pickle.load(documents_f)\n",
        "    documents_f.close()\n",
        "    print(str(len(testing_files)) + \" testing files loaded.....\")\n",
        "\n",
        "    testing_featureset = []\n",
        "\n",
        "    for index in range(len(testing_files)):\n",
        "        testing_featureset.append(\n",
        "            [find_features(word_features, testing_files[index][2]),\n",
        "             testing_files[index][1]])\n",
        "    # one bug was here fixed, \n",
        "    save_featuresets = open(MASTER_PATH + \"/src/feature_modeling/testing_featureset.pickle\", \"wb\")\n",
        "    pickle.dump(testing_featureset, save_featuresets)\n",
        "    save_featuresets.close()\n",
        "    print(\"testing featuresets saved.....\")\n",
        "    print(\"===============>\")\n",
        "\n",
        "    for t_index in range(10):\n",
        "        documents_f = open(MASTER_PATH + \"/src/T&T/training\" + str(t_index + 1) +\".pickle\", \"rb\")\n",
        "        training_files = pickle.load(documents_f)\n",
        "        documents_f.close()\n",
        "        print(str(len(training_files)) + \" training files loaded.....\")\n",
        "\n",
        "        training_featureset = []\n",
        "\n",
        "        for index in range(len(training_files)):\n",
        "            training_featureset.append(\n",
        "                [find_features(word_features, training_files[index][2]),\n",
        "                 training_files[index][1]])\n",
        "        # one bug was here the path for saving feature_modeling.pickle file was /src \n",
        "        # so I change this path with /src/feature_modeling/ which we will use next part.\n",
        "        save_featuresets = open(MASTER_PATH + \"/src/feature_modeling/feature_modeling\" + str(t_index + 1) + \".pickle\",\"wb\")\n",
        "        pickle.dump(training_featureset, save_featuresets)\n",
        "        save_featuresets.close()\n",
        "        print(\"training featuresets \" + str(t_index + 1) + \" saved.....\")\n",
        "        print(\"===============>\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # dataset = load_dataset()\n",
        "    features = load_features()\n",
        "    # saperate_training_and_testing_data(dataset)\n",
        "    make_featuresets(features)\n",
        "\n"
      ],
      "metadata": {
        "id": "vHxaTw7hJllB",
        "outputId": "5258a8ff-97b4-4dc0-fcb6-32713d0554da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features of length  5000  is loaded.....\n",
            "1336 testing files loaded.....\n",
            "testing featuresets saved.....\n",
            "===============>\n",
            "1200 training files loaded.....\n",
            "training featuresets 1 saved.....\n",
            "===============>\n",
            "1200 training files loaded.....\n",
            "training featuresets 2 saved.....\n",
            "===============>\n",
            "1200 training files loaded.....\n",
            "training featuresets 3 saved.....\n",
            "===============>\n",
            "1200 training files loaded.....\n",
            "training featuresets 4 saved.....\n",
            "===============>\n",
            "1200 training files loaded.....\n",
            "training featuresets 5 saved.....\n",
            "===============>\n",
            "1200 training files loaded.....\n",
            "training featuresets 6 saved.....\n",
            "===============>\n",
            "1200 training files loaded.....\n",
            "training featuresets 7 saved.....\n",
            "===============>\n",
            "1200 training files loaded.....\n",
            "training featuresets 8 saved.....\n",
            "===============>\n",
            "1200 training files loaded.....\n",
            "training featuresets 9 saved.....\n",
            "===============>\n",
            "1228 training files loaded.....\n",
            "training featuresets 10 saved.....\n",
            "===============>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. refactoring_prediction.py (ATTENTION)"
      ],
      "metadata": {
        "id": "OS6iLQZZKvVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "import pickle\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# classifiers = ['SVM', 'NB', 'MNB', 'BNB', 'LR', 'RF']\n",
        "classifiers = ['SVM']\n",
        "# data loading\n",
        "featuresets_f = open(MASTER_PATH + \"/src/feature_modeling/testing_featureset.pickle\", \"rb\")\n",
        "testing_set = pickle.load(featuresets_f)\n",
        "featuresets_f.close()\n",
        "print(\"lenght of testing dataset: \", len(testing_set))\n",
        "\n",
        "for t_index in range(10):\n",
        "\n",
        "    print(\"iteration \" + str(t_index + 1))\n",
        "    featuresets_f = open(MASTER_PATH + \"/src/feature_modeling/feature_modeling\" + str(t_index + 1) + \".pickle\", \"rb\")\n",
        "    training = pickle.load(featuresets_f)\n",
        "    featuresets_f.close()\n",
        "    print(\"lenght of training dataset: \", len(training))\n",
        "\n",
        "    for cls in classifiers:\n",
        "        if cls == 'SVM':\n",
        "            classifier = SklearnClassifier(LinearSVC())\n",
        "            classifier.train(training)\n",
        "        elif cls == 'NB':\n",
        "            classifier = nltk.NaiveBayesClassifier.train(training)\n",
        "            classifier.train(training)\n",
        "        elif cls == 'MNB':\n",
        "            classifier = SklearnClassifier(MultinomialNB())\n",
        "            classifier.train(training)\n",
        "        elif cls == 'BNB':\n",
        "            classifier = SklearnClassifier(BernoulliNB())\n",
        "            classifier.train(training)\n",
        "        elif cls == 'LR':\n",
        "            classifier = SklearnClassifier(LogisticRegression())\n",
        "            classifier.train(training)\n",
        "        elif cls == 'RF':\n",
        "            classifier = SklearnClassifier(RandomForestClassifier())\n",
        "            classifier.train(training)\n",
        "        # prediction\n",
        "        y_true, y_pred = [], []\n",
        "\n",
        "        for i, (feats, label_true) in enumerate(testing_set):\n",
        "            label_pred = classifier.classify(feats)\n",
        "            y_true.append(label_true)\n",
        "            y_pred.append(label_pred)\n",
        "\n",
        "        # save_classifier = open(\"./trained_classifiers/LRall.pickle\", \"wb\")\n",
        "        save_classifier = open(MASTER_PATH + \"/src/trained_classifiers/\" + cls + str(t_index + 1) + \".pickle\", \"wb\")\n",
        "        pickle.dump(classifier, save_classifier)\n",
        "        save_classifier.close()\n",
        "\n",
        "        # save_classifier = open(\"./y_true_pred/y_true_LRall.pickle\", \"wb\")\n",
        "        save_classifier = open(MASTER_PATH + \"/src/y_true_pred/y_true_\" + cls + str(t_index + 1) +  \".pickle\", \"wb\")\n",
        "        pickle.dump(y_true, save_classifier)\n",
        "        save_classifier.close()\n",
        "\n",
        "        # save_classifier = open(\"./y_true_pred/y_pred_LRall.pickle\", \"wb\")\n",
        "        save_classifier = open(MASTER_PATH + \"/src/y_true_pred/y_pred_\" + cls + str(t_index + 1) +  \".pickle\", \"wb\")\n",
        "        pickle.dump(y_pred, save_classifier)\n",
        "        save_classifier.close()\n",
        "\n",
        "        print(cls + \" for iteration \" + str(t_index + 1) + \" done.....\")\n",
        "        print(\"=========>>\")"
      ],
      "metadata": {
        "id": "vrHFXhb_KzkB",
        "outputId": "b2b320f4-068b-4433-a750-804ea0774e5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lenght of testing dataset:  1336\n",
            "iteration 1\n",
            "lenght of training dataset:  1200\n",
            "SVM for iteration 1 done.....\n",
            "=========>>\n",
            "iteration 2\n",
            "lenght of training dataset:  1200\n",
            "SVM for iteration 2 done.....\n",
            "=========>>\n",
            "iteration 3\n",
            "lenght of training dataset:  1200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM for iteration 3 done.....\n",
            "=========>>\n",
            "iteration 4\n",
            "lenght of training dataset:  1200\n",
            "SVM for iteration 4 done.....\n",
            "=========>>\n",
            "iteration 5\n",
            "lenght of training dataset:  1200\n",
            "SVM for iteration 5 done.....\n",
            "=========>>\n",
            "iteration 6\n",
            "lenght of training dataset:  1200\n",
            "SVM for iteration 6 done.....\n",
            "=========>>\n",
            "iteration 7\n",
            "lenght of training dataset:  1200\n",
            "SVM for iteration 7 done.....\n",
            "=========>>\n",
            "iteration 8\n",
            "lenght of training dataset:  1200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM for iteration 8 done.....\n",
            "=========>>\n",
            "iteration 9\n",
            "lenght of training dataset:  1200\n",
            "SVM for iteration 9 done.....\n",
            "=========>>\n",
            "iteration 10\n",
            "lenght of training dataset:  1228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM for iteration 10 done.....\n",
            "=========>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. multi-label_classifiers.py\n"
      ],
      "metadata": {
        "id": "usIbjm2VQTHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyodbc"
      ],
      "metadata": {
        "id": "aXa0lax7Q44F",
        "outputId": "259c1a17-019b-42e8-8693-c5657bdf4a23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyodbc in /usr/local/lib/python3.8/dist-packages (4.0.35)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC, LinearSVR\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import hamming_loss, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pyodbc\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "\n",
        "df_text = pd.read_csv(MASTER_PATH + '/src/dataset/TextPreprocessed.csv', encoding='iso-8859-1')\n",
        "# print(df_text.head())\n",
        "df_tags = pd.read_csv(MASTER_PATH + '/src/dataset/Tag.csv', encoding='iso-8859-1')\n",
        "\n",
        "num_classes = 14\n",
        "grouped_tags = df_tags.groupby(\"Tag\").size().reset_index(name='count')\n",
        "most_common_tags = grouped_tags.nlargest(num_classes, columns=\"count\")\n",
        "df_tags.Tag = df_tags.Tag.apply(lambda tag : tag if tag in most_common_tags.Tag.values else None)\n",
        "df_tags = df_tags.dropna()\n",
        "\n",
        "counts = df_tags.Tag.value_counts()\n",
        "firstlast = counts[:5].append(counts[-5:])\n",
        "firstlast.reset_index(name=\"count\")\n",
        "\n",
        "# print(firstlast)\n",
        "\n",
        "def tags_for_question(question_id):\n",
        "    return df_tags[df_tags['Id'] == question_id].Tag.values\n",
        "\n",
        "def add_tags_column(row):\n",
        "    row['Tags'] = tags_for_question(row['Id'])\n",
        "    return row\n",
        "\n",
        "df_questions = df_text.apply(add_tags_column, axis=1)\n",
        "\n",
        "multilabel_binarizer = MultiLabelBinarizer()\n",
        "multilabel_binarizer.fit(df_questions.Tags)\n",
        "Y = multilabel_binarizer.transform(df_questions.Tags)\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "X_counts = count_vect.fit_transform(df_questions.Text.values.astype('U'))\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
        "\n",
        "ros = RandomOverSampler(random_state=9200)\n",
        "\n",
        "X_tfidf_resampled, Y_tfidf_resampled = ros.fit_resample(X_tfidf, Y)\n",
        "\n",
        "x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf_resampled, Y_tfidf_resampled, test_size=0.10, random_state=9200)\n",
        "\n",
        "\n",
        "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
        "    print(y_true.shape[0])\n",
        "    print(y_pred)\n",
        "\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set(np.where(y_true[i])[0])\n",
        "        set_pred = set(np.where(y_pred[i])[0])\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            # tmp_a = len(set_true.union(set_pred))\n",
        "            tmp_a = len(set_true.intersection(set_pred))/float(len(set_true.union(set_pred)) )\n",
        "        acc_list.append(tmp_a)\n",
        "    # print(acc_list)\n",
        "    return np.mean(acc_list)\n",
        "\n",
        "def print_score(y_pred, clf):\n",
        "    print(\"Clf: \", clf.__class__.__name__)\n",
        "    # print(\"Hamming loss: {}\".format(hamming_loss(y_test_tfidf, y_pred)))\n",
        "    print(\"Hamming score: {}\".format(hamming_score(y_test_tfidf, y_pred)))\n",
        "    # print('Subset accuracy: {0}'.format(accuracy_score(y_test_tfidf, y_pred, normalize=True, sample_weight=None)))\n",
        "    # print('Subset precision: {0}'.format(precision_score(y_test_tfidf, y_pred, average='samples')))\n",
        "    print(\"---\")\n",
        "\n",
        "# sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=6, tol=None)\n",
        "#lr = LogisticRegression()\n",
        "#mn = MultinomialNB()\n",
        "svm = LinearSVC()\n",
        "\n",
        "for classifier in [svm]:\n",
        "    clf = OneVsRestClassifier(classifier)\n",
        "    clf.fit(x_train_tfidf, y_train_tfidf)\n",
        "    y_pred = clf.predict(x_test_tfidf)\n",
        "    # print_score(y_pred, classifier)\n",
        "    print(classification_report(y_test_tfidf, y_pred))"
      ],
      "metadata": {
        "id": "WupLfCwbQaTD",
        "outputId": "5da8b017-1f6f-43e0-b7da-aabf275f8944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.91      0.92       272\n",
            "           1       0.50      0.06      0.11       232\n",
            "           2       0.91      0.92      0.92       245\n",
            "           3       0.86      0.72      0.78       248\n",
            "           4       0.92      0.86      0.89       268\n",
            "           5       0.87      0.64      0.74       238\n",
            "           6       0.89      0.72      0.80       255\n",
            "           7       0.89      0.49      0.63       282\n",
            "           8       0.87      0.95      0.91       265\n",
            "           9       0.90      0.78      0.84       253\n",
            "          10       0.88      0.89      0.88       273\n",
            "          11       0.90      0.87      0.89       252\n",
            "          12       0.93      0.63      0.75       296\n",
            "          13       0.78      0.27      0.40       271\n",
            "\n",
            "   micro avg       0.89      0.70      0.78      3650\n",
            "   macro avg       0.86      0.69      0.75      3650\n",
            "weighted avg       0.86      0.70      0.75      3650\n",
            " samples avg       0.70      0.70      0.70      3650\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Set the number of folds and random seed (if desired)\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "# Initialize lists to store the results\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "# Iterate over the folds\n",
        "for train_index, test_index in kf.split(df_questions):\n",
        "    # Split the data into training and test sets\n",
        "    x_train, x_test = df_questions.Text[train_index], df_questions.Text[test_index]\n",
        "    y_train, y_test = Y[train_index], Y[test_index]\n",
        "    \n",
        "    # Preprocess the data, transform it, and oversample it\n",
        "    count_vect = CountVectorizer()\n",
        "    x_train_counts = count_vect.fit_transform(x_train.values.astype('U'))\n",
        "    x_test_counts = count_vect.transform(x_test.values.astype('U'))\n",
        "    \n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "    x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)\n",
        "    x_test_tfidf = tfidf_transformer.transform(x_test_counts)\n",
        "    \n",
        "    ros = RandomOverSampler(random_state=9000)\n",
        "    x_train_tfidf_resampled, y_train_tfidf_resampled = ros.fit_resample(x_train_tfidf, y_train)\n",
        "    x_test_tfidf_resampled, y_test_tfidf_resampled = ros.fit_resample(x_test_tfidf, y_test)\n",
        "\n",
        "    # Train the model on the training data\n",
        "    clf = OneVsRestClassifier(LogisticRegression())\n",
        "    clf.fit(x_train_tfidf_resampled, y_train_tfidf_resampled)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = clf.predict(x_test_tfidf_resampled)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(y_test_tfidf_resampled, y_pred)\n",
        "    precision = precision_score(y_test_tfidf_resampled, y_pred, average='micro')\n",
        "    recall = recall_score(y_test_tfidf_resampled, y_pred, average='micro')\n",
        "    f1 = f1_score(y_test_tfidf_resampled, y_pred, average='micro')\n",
        "\n",
        "    # Append the results to the lists\n",
        "    accuracies.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Calculate the mean and standard deviation of the evaluation metrics\n",
        "mean_accuracy = np.mean(accuracies)\n",
        "std_accuracy = np.std(accuracies)\n",
        "mean_precision = np.mean(precisions)\n",
        "std_precision = np.std(precisions)\n",
        "mean_recall = np.mean(recalls)\n",
        "std_recall = np.std(recalls)\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "std_f1 = np.std(f1_scores)\n",
        "\n",
        "# Print the results\n",
        "print(f'Accuracy: {mean_accuracy:.3f} +/- {std_accuracy:.3f}')\n",
        "print(f'Precision: {mean_precision:.3f} +/- {std_precision:.3f}')\n",
        "print(f'Recall: {mean_recall:.3f} +/- {std_recall:.3f}')\n",
        "print(f'F1 score: {mean_f1:.3f} +/- {std_f1:.3f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS9kGeDijDY2",
        "outputId": "269cf7dd-6249-4d31-ae4b-9ab43347370d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.008 +/- 0.003\n",
            "Precision: 0.084 +/- 0.031\n",
            "Recall: 0.008 +/- 0.003\n",
            "F1 score: 0.014 +/- 0.005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hlGLK2BJjKUc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}